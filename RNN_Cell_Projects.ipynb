{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to this Jupyter notebook, where we delve into the fascinating world of Recurrent Neural Networks (RNNs) using PyTorch. RNNs are a class of neural networks that are pivotal in processing sequential data, making them crucial for tasks like language modeling, time series analysis, and more. PyTorch, being a powerful and flexible deep learning library, provides efficient implementations of RNNs, allowing for rapid prototyping and experimentation.\n",
    "\n",
    "**Objective of This Notebook**\n",
    "The primary objective of this notebook is to learn about RNN cell:\n",
    "\n",
    "1. PyTorch's Built-in RNN Cell: We will use PyTorch's torch.nn.RNN module to create an RNN instance. This serves as our baseline model, representing the standard implementation in the field.\n",
    "\n",
    "2. Custom-Built RNN Cell: We will construct our own RNN cell from scratch. This exercise is aimed at gaining deeper insights into the mechanics of RNNs and understanding the nuances of recurrent neural processing.\n",
    "\n",
    "**Key Aspects of Comparison**\n",
    "Our comparison will focus on the following aspects:\n",
    "\n",
    "1. Output Dimensions: Ensuring that both models yield outputs with consistent dimensions is crucial. This includes testing across various scenarios:\n",
    "    - Input without batch dimension\n",
    "    - Input with batch dimension\n",
    "    - Variations when the batch dimension is either the first or second axis\n",
    "\n",
    "**Learning Outcomes**\n",
    "By the end of this notebook, we aim to achieve a comprehensive understanding of:\n",
    "- The architecture and functionality of RNN cells\n",
    "- How PyTorch implements RNNs and manages data dimensions\n",
    "- The intricacies involved in building an RNN cell from the ground up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Built-in RNN Cell\n",
    "\n",
    "In this notebook, we are going to present the architecture of a RNN cell from PyTorch. If we look at the documentation of PyTorch RNN cell (https://pytorch.org/docs/stable/generated/torch.nn.RNN.html).\n",
    "\n",
    "\n",
    "The PyTorch RNN cell implements a multi-layer Elman RNN, which can use either a tanh or ReLU activation function. The core behavior of an RNN cell is described by the following equation:\n",
    "\n",
    "$$ h_{t} = tanh(x_{t}*W_{ih}^T + b_{ih} + h_{t-1}*W_{hh}^T + b_{hh}) $$\n",
    "\n",
    "This equation encapsulates the essence of the RNN's recurrent nature. It combines two linear transformations - one applied to the current input $x_{t}$ and the other to the previous hidden state $h_{t-1}$. \n",
    "\n",
    "The weights $W_{ih}$ and $W_{hh}$ represent the input-to-hidden and hidden-to-hidden connections, respectively, while $b_{ih}$ and $b_{hh}$ are their corresponding biases.\n",
    "\n",
    "The process can be broken into steps:\n",
    "- Initialize\n",
    "    - $h = 0$\n",
    "- For each time step $t$ in  the sequence:\n",
    "    - $x = x_{t}*W_{ih}^T + b_{ih}$\n",
    "    - $h = h_{t-1}*W_{hh}^T + b_{hh}$\n",
    "    - $c = x + h$\n",
    "    - $h_{t} = tanh(c)$\n",
    "\n",
    "In essence, this equation describes how an RNN cell combines the current input with the previous hidden state to generate the current hidden state to concatenante with the next input. This process is repeated across each time step in the input sequence, allowing the RNN to maintain a form of 'memory' of past inputs. This memory is crucial for tasks where context and sequence order are important, such as language modeling, speech recognition, and time series analysis. \n",
    "\n",
    "According to the documentation of Pytorch's RNN cell, we need to respect the folowing sizes:\n",
    "- *S*: Sequence Length - the number of time steps in the input.\n",
    "- *I*: Input Size - the dimension of the input vector at each time step.\n",
    "- *H*: Hidden Size - the size of the hidden state vector, a key parameter indicating the network's memory capacity.\n",
    "- *B*: Batch Size - the number of sequences processed simultaneously.\n",
    "- *L*: Number of Layers - layers in the RNN.\n",
    "- *D*: Direction - 2 for bidirectional RNNs (forward and backward), else 1.\n",
    "\n",
    "- **Inputs**\n",
    "    - input $x_{t}$\n",
    "        - Unbatched (S, I)\n",
    "        - Batched   (S, B, I) or (B,S,I)\n",
    "    - hidden $h_{t-1}$\n",
    "        - Unbatched (DL, H) \n",
    "        - Batched   (DL, B, H) or (B, DL, H)\n",
    "- **Outputs**\n",
    "    - output $o_{t}$\n",
    "        - Unbatched (S, DH)\n",
    "        - Batched   (S, B, DH) or (B, S, DH) \n",
    "    - hidden $h_{t}$\n",
    "        - Unbatched (DL, H) \n",
    "        - Batched   (DL, B, H) or (B, DL, H)\n",
    "\n",
    "Output Dimensions (S, D*H): The output of an RNN cell is not directly the final prediction (e.g., a class or a word) but rather the representation of the sequence after processing through the RNN layers. Each time step outputs a vector of size DH, which can be further processed or used as input to another layer or network.\n",
    "\n",
    "In RNNs, the terms \"hidden state\" and \"output\" are often used interchangeably, but they can have different meanings depending on context:\n",
    "\n",
    "Hidden State $h_{t}$: Represents the memory of the network, carrying information across time steps. It's updated at each time step based on the current input and the previous hidden state.\n",
    "\n",
    "Output $o_{t}$: While the hidden state is the RNN's internal memory, the output at each time step can be a transformed version of this hidden state, tailored for specific tasks. For example, in many applications, a linear layer followed by an activation function is applied to the hidden state to produce the output.\n",
    "\n",
    "The last thing to say is the possibility to set the batch dimension at first or second place. It depends on the convenience, how does it integrated, preferences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-In Pytorch RNN example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# inputs and parameters\n",
    "input_size = 13\n",
    "sequence_length = 17\n",
    "hidden_size = 29\n",
    "num_layers = 7\n",
    "nonlinearity = 'tanh'\n",
    "bias = True\n",
    "batch_first = False\n",
    "batch_size = 32\n",
    "dropout = 0.1\n",
    "dropout = dropout if num_layers > 1 else 0\n",
    "bidirectional = True\n",
    "D = 2 if bidirectional else 1\n",
    "\n",
    "# model rnn built in pytorch\n",
    "rnn = nn.RNN(\n",
    "    input_size, \n",
    "    hidden_size, \n",
    "    num_layers, \n",
    "    nonlinearity=nonlinearity, \n",
    "    bias=bias, \n",
    "    batch_first=batch_first, \n",
    "    dropout=dropout, \n",
    "    bidirectional=bidirectional\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape:  torch.Size([17, 58])\n",
      "hidden state shape:  torch.Size([14, 29])\n"
     ]
    }
   ],
   "source": [
    "# Unbatched input\n",
    "input = torch.randn(sequence_length, input_size)\n",
    "\n",
    "# run model\n",
    "output, hn = rnn(input)\n",
    "\n",
    "# print output and hidden state shape\n",
    "print(\"output shape: \", output.shape)\n",
    "print(\"hidden state shape: \", hn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch first:  False\n",
      "output shape:  torch.Size([17, 32, 58])\n",
      "hidden state shape:  torch.Size([14, 32, 29])\n"
     ]
    }
   ],
   "source": [
    "# Batched input\n",
    "if batch_first:\n",
    "    input = torch.randn(batch_size, sequence_length, input_size)\n",
    "else:\n",
    "    input = torch.randn(sequence_length, batch_size, input_size)\n",
    "\n",
    "# run model\n",
    "output, hn = rnn(input)\n",
    "\n",
    "# print output and hidden state shape\n",
    "print('Batch first: ', batch_first)\n",
    "print(\"output shape: \", output.shape)\n",
    "print(\"hidden state shape: \", hn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ve got here the sizes of output and hidden states tensors and we need to get the same sizes from the RNN cell developed from scratch. Here we can play with the sizes definition and parameters for printing different outputs tensors. All of scenatios must work with our implemented solutions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN From Scratch\n",
    "\n",
    "Let s try to build from fundation NN cell this RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Approximation \n",
    "\n",
    "The RNN cell is applying $ h_{t} = tanh(x_{t}*W_{ih}^T + b_{ih} + h_{t-1}*W_{hh}^T + b_{hh}) $ which actually can be understood as a product of 2 matrices. Let s consider the matrix $X = \\begin{pmatrix} x_{t} & h_{t-1} \\end{pmatrix}$ and $W = \\begin{pmatrix} W_{ih} \\\\ W_{hh} \\end{pmatrix}$, their product implies the previous sum. \n",
    "\n",
    "So in first approximation the RNN cell applies a linear function on input and previous hidden state concatenate together. By deduction we need to define the input size as the sum of sizes of $x_{t}$ and $h_{t}$.\n",
    "\n",
    "In this context of first approximation we need to implement only the root architecture, so we consider that:\n",
    "1. number of layers is 1\n",
    "2. the RNN cell is not bidirectional, D=1\n",
    "3. bias is always True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, nonlinearity='tanh', batch_first=False):\n",
    "        super(BasicRNN, self).__init__()\n",
    "        # RNN Cell parameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        # Define the layers for input + hidden\n",
    "        self.rnn_layer = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, h_0=None):\n",
    "        # Adjust for shape without batch dimension to get 3 dimensions\n",
    "        if len(x.shape) == 2:\n",
    "            if self.batch_first:\n",
    "                x = x.unsqueeze(0) # (seq, feature) -> (batch, seq, feature)\n",
    "            else:\n",
    "                x = x.unsqueeze(1) # (seq, feature) -> (seq, batch, feature)\n",
    "\n",
    "        # Adjust for batch_first\n",
    "        if self.batch_first:\n",
    "            x = x.transpose(0, 1) # (batch, seq, feature) -> (seq, batch, feature)\n",
    "\n",
    "        # Initialize hidden state\n",
    "        seq_len, batch_size, _ = x.size()\n",
    "        if h_0 is None:\n",
    "            h_0 = torch.zeros(batch_size, self.hidden_size) # num_layers = 1, squeezed on dim 0\n",
    "\n",
    "        # Process the sequence\n",
    "        outputs = []\n",
    "        h_t = h_0\n",
    "\n",
    "        # At each time step\n",
    "        for t in range(seq_len):\n",
    "            # Concatenate input and hidden state\n",
    "            combined = torch.cat((h_t, x[t]), dim=1)\n",
    "\n",
    "            # Calculate the hidden state\n",
    "            h_t = self.rnn_layer(combined)\n",
    "\n",
    "            # Apply nonlinearity\n",
    "            h_t = torch.tanh(h_t) if self.nonlinearity == 'tanh' else torch.relu(h_t)\n",
    "\n",
    "            # Append to outputs\n",
    "            outputs.append(h_t)\n",
    "\n",
    "        # Convert list to tensor\n",
    "        output = torch.stack(outputs)\n",
    "\n",
    "        # Adjust for batch_first\n",
    "        if self.batch_first:\n",
    "            output = output.transpose(0, 1)\n",
    "\n",
    "        if batch_size == 1:\n",
    "            if self.batch_first:\n",
    "                output = output.squeeze(0)\n",
    "            else:\n",
    "                output = output.squeeze(1)\n",
    "\n",
    "        # if batch_size > 1: unsqueeze hidden state because num_layers = 1\n",
    "        if batch_size > 1:\n",
    "            h_t = h_t.unsqueeze(0)\n",
    "            \n",
    "        return output, h_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 1\n",
    "nonlinearity = 'tanh'\n",
    "batch_first = True\n",
    "bidirectional = False\n",
    "D = 2 if bidirectional else 1\n",
    "\n",
    "# model rnn\n",
    "basic_rnn = BasicRNN(\n",
    "    input_size, \n",
    "    hidden_size, \n",
    "    nonlinearity=nonlinearity, \n",
    "    batch_first=batch_first\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape:  torch.Size([17, 29])\n",
      "hidden state shape:  torch.Size([1, 29])\n"
     ]
    }
   ],
   "source": [
    "# Unbatched input\n",
    "input = torch.randn(sequence_length, input_size)\n",
    "\n",
    "# run model\n",
    "output, hn = basic_rnn(input)\n",
    "\n",
    "# print output and hidden state shape\n",
    "print(\"output shape: \", output.shape)\n",
    "print(\"hidden state shape: \", hn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch first:  True\n",
      "output shape:  torch.Size([32, 17, 29])\n",
      "hidden state shape:  torch.Size([1, 32, 29])\n"
     ]
    }
   ],
   "source": [
    "# Batched input\n",
    "if batch_first:\n",
    "    input = torch.randn(batch_size, sequence_length, input_size)\n",
    "else:\n",
    "    input = torch.randn(sequence_length, batch_size, input_size)\n",
    "\n",
    "# run model\n",
    "output, hn = basic_rnn(input)\n",
    "\n",
    "# print output and hidden state shape\n",
    "print('Batch first: ', batch_first)\n",
    "print(\"output shape: \", output.shape)\n",
    "print(\"hidden state shape: \", hn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Approximation \n",
    "\n",
    "Now I want to add the ability to multiply the number of layers.\n",
    "\n",
    "For multilayers RNN, the RNN cells are stacked together to obtain a bigger model capable to learn more pattern from the sequential input. To implement this stack version we will use the Basic RNN. \n",
    "\n",
    "The first layer receives the initial input sequence then the subsequent layers are fed with the output (not the hidden state) of the previous layer. Each layer has its own hidden state, which it updates at each time step based on its current input and its previous hidden state.\n",
    "\n",
    "The output comes from the last layer of the RNN. It's the sequence of transformed representations at each time step.\n",
    "The hidden states are the final hidden states of each layer after processing the entire input sequence.\n",
    "\n",
    "In a forward pass, only the output of each layer (except for the last one) is fed as input to the next layer.\n",
    "\n",
    "Example: Consider a 2-layer RNN. The first layer receives the input sequence, processes it, and its output is then passed to the second layer. The second layer processes this input (which is the output of the first layer) and produces its own output and hidden state. The RNN as a whole returns the output of the second layer and the hidden states of both layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, nonlinearity='tanh', batch_first=False):\n",
    "        super(StackedRNN, self).__init__()\n",
    "        # Stacked RNN parameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        # Initialize layers of BasicRNN\n",
    "        self.layers = nn.ModuleList([BasicRNN(input_size if i == 0 else hidden_size, \n",
    "                                              hidden_size, \n",
    "                                              nonlinearity=nonlinearity, \n",
    "                                              batch_first=batch_first) for i in range(num_layers)])\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        \n",
    "        # list to store hidden states for each layer\n",
    "        hiddens = []\n",
    "\n",
    "        # loop through layers\n",
    "        for i in range(self.num_layers):\n",
    "            # pass through layer\n",
    "            output, hidden = self.layers[i](input)\n",
    "\n",
    "            # output of layer is input of next layer\n",
    "            input = output\n",
    "\n",
    "            # append hidden state\n",
    "            hiddens.append(hidden)\n",
    "        \n",
    "        # stack hidden states - convert to tensor\n",
    "        hiddens = torch.stack(hiddens).squeeze(1)\n",
    "\n",
    "        return output, hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 7\n",
    "nonlinearity = 'tanh'\n",
    "batch_first = True\n",
    "bidirectional = False\n",
    "D = 2 if bidirectional else 1\n",
    "\n",
    "# model rnn\n",
    "stack_rnn = StackedRNN(\n",
    "    input_size, \n",
    "    hidden_size, \n",
    "    num_layers,\n",
    "    nonlinearity=nonlinearity, \n",
    "    batch_first=batch_first\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape:  torch.Size([17, 29])\n",
      "hidden state shape:  torch.Size([7, 29])\n"
     ]
    }
   ],
   "source": [
    "# Unbatched input\n",
    "input = torch.randn(sequence_length, input_size)\n",
    "\n",
    "# run model\n",
    "output, hn = stack_rnn(input)\n",
    "\n",
    "# print output and hidden state shape\n",
    "print(\"output shape: \", output.shape)\n",
    "print(\"hidden state shape: \", hn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch first:  True\n",
      "output shape:  torch.Size([32, 17, 29])\n",
      "hidden state shape:  torch.Size([7, 32, 29])\n"
     ]
    }
   ],
   "source": [
    "# Batched input\n",
    "if batch_first:\n",
    "    input = torch.randn(batch_size, sequence_length, input_size)\n",
    "else:\n",
    "    input = torch.randn(sequence_length, batch_size, input_size)\n",
    "\n",
    "# run model\n",
    "output, hn = stack_rnn(input)\n",
    "\n",
    "# print output and hidden state shape\n",
    "print('Batch first: ', batch_first)\n",
    "print(\"output shape: \", output.shape)\n",
    "print(\"hidden state shape: \", hn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Approximation\n",
    "\n",
    "Now let s integrate the bidirectionality\n",
    "\n",
    "Bidirectionality in the context of Recurrent Neural Networks (RNNs) is a technique designed to enhance the model's understanding of the input data by providing it with information from both past (backward) and future (forward) directions. \n",
    "\n",
    "It  consists of two separate RNNs â€“ a forward RNN and a backward RNN. The forward RNN processes the sequence from the start to the end, while the backward RNN processes it from the end to the start.\n",
    "\n",
    "At each time step, the outputs of the forward and backward RNNs are combined. This combination can be done in various ways, such as concatenation or summation, depending on the desired outcome.\n",
    "\n",
    "The combined output at each time step then forms the overall output of the bidirectional RNN. This output can be used for further processing or making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BidirectionalStackedRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, nonlinearity='tanh', batch_first=False):\n",
    "        super(BidirectionalStackedRNN, self).__init__()\n",
    "        # Bidirectional Stacked RNN parameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        # Initialize layers of StackedRNN for forward and backward directions\n",
    "        self.forward_layers = StackedRNN(input_size, hidden_size, num_layers,nonlinearity=nonlinearity, batch_first=batch_first)\n",
    "        self.backward_layers = StackedRNN(input_size, hidden_size, num_layers,nonlinearity=nonlinearity, batch_first=batch_first)\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        \n",
    "        # Forward pass through forward layers\n",
    "        forward_outputs, forward_hiddens = self.forward_layers(input, hidden)\n",
    "\n",
    "        # Reverse the input sequence\n",
    "        backward_outputs, backward_hiddens = self.backward_layers(input.flip(0), hidden)\n",
    "\n",
    "        # Concatenate or add the outputs from both directions\n",
    "        output = torch.cat((forward_outputs, backward_outputs), dim=-1)\n",
    "\n",
    "        # Concatenate the hidden states from both directions for each layer\n",
    "        hiddens = torch.cat((forward_hiddens, backward_hiddens), dim=0)\n",
    "\n",
    "        return output, hiddens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 7\n",
    "nonlinearity = 'tanh'\n",
    "batch_first = False\n",
    "\n",
    "\n",
    "# model rnn\n",
    "bi_stack_rnn = BidirectionalStackedRNN(\n",
    "    input_size, \n",
    "    hidden_size, \n",
    "    num_layers,\n",
    "    nonlinearity=nonlinearity, \n",
    "    batch_first=batch_first\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape:  torch.Size([17, 58])\n",
      "hidden state shape:  torch.Size([14, 29])\n"
     ]
    }
   ],
   "source": [
    "# Unbatched input\n",
    "input = torch.randn(sequence_length, input_size)\n",
    "\n",
    "# run model\n",
    "output, hn = bi_stack_rnn(input)\n",
    "\n",
    "# print output and hidden state shape\n",
    "print(\"output shape: \", output.shape)\n",
    "print(\"hidden state shape: \", hn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch first:  False\n",
      "output shape:  torch.Size([17, 32, 58])\n",
      "hidden state shape:  torch.Size([14, 32, 29])\n"
     ]
    }
   ],
   "source": [
    "# Batched input\n",
    "if batch_first:\n",
    "    input = torch.randn(batch_size, sequence_length, input_size)\n",
    "else:\n",
    "    input = torch.randn(sequence_length, batch_size, input_size)\n",
    "\n",
    "# run model\n",
    "output, hn = bi_stack_rnn(input)\n",
    "\n",
    "# print output and hidden state shape\n",
    "print('Batch first: ', batch_first)\n",
    "print(\"output shape: \", output.shape)\n",
    "print(\"hidden state shape: \", hn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forth Approximation\n",
    "\n",
    "Now only remains the dropout. \n",
    "\n",
    "Dropout is a regularization technique used to prevent overfitting in neural networks, including multi-layer Recurrent Neural Networks (RNNs). In the context of multi-layer RNNs, dropout can be applied in various ways to reduce the likelihood of co-adaptation of neurons and to improve the generalization capability of the model.\n",
    "\n",
    "One common approach is to apply dropout between the RNN layers in a stacked RNN. This means dropout is applied to the output of each RNN layer before it is used as input to the next layer. It's important to note that dropout is typically not applied to the recurrent connections (the connections from the hidden state of a layer at one time step to the hidden state of the same layer at the next time step) but to the connections between layers.\n",
    "\n",
    "Dropout can also be applied to the input layer (before the first RNN layer) and/or the output layer (after the last RNN layer), depending on the specific architecture and the problem at hand.\n",
    "\n",
    "A variant of dropout, called variational dropout, applies the same dropout mask at each time step, instead of varying it across time steps. This approach is often used in RNNs to maintain consistency in dropout application across the temporal connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropOutStackedRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, nonlinearity='tanh', batch_first=False, dropout=0.0):\n",
    "        super(DropOutStackedRNN, self).__init__()\n",
    "        # Stacked RNN parameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.batch_first = batch_first\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Initialize layers of BasicRNN\n",
    "        self.layers = nn.ModuleList([BasicRNN(input_size if i == 0 else hidden_size, \n",
    "                                              hidden_size, \n",
    "                                              nonlinearity=nonlinearity, \n",
    "                                              batch_first=batch_first) for i in range(num_layers)])\n",
    "        \n",
    "        # Initialize dropout layer\n",
    "        self.dropout = nn.Dropout(dropout) if num_layers > 1 else nn.Identity()\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        \n",
    "        # list to store hidden states for each layer\n",
    "        hiddens = []\n",
    "\n",
    "        # loop through layers\n",
    "        for i in range(self.num_layers):\n",
    "            # pass through layer\n",
    "            output, hidden = self.layers[i](input)\n",
    "\n",
    "            # Apply dropout to the output of each layer except the last layer\n",
    "            if i < self.num_layers - 1:\n",
    "                input = self.dropout(output)\n",
    "            else:\n",
    "                input = output\n",
    "\n",
    "            # append hidden state\n",
    "            hiddens.append(hidden)\n",
    "        \n",
    "        # stack hidden states - convert to tensor\n",
    "        hiddens = torch.stack(hiddens).squeeze(1)\n",
    "\n",
    "        return output, hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 7\n",
    "nonlinearity = 'tanh'\n",
    "batch_first = False\n",
    "dropout = 0.1\n",
    "\n",
    "# model rnn\n",
    "d_stack_rnn = DropOutStackedRNN(\n",
    "    input_size, \n",
    "    hidden_size, \n",
    "    num_layers,\n",
    "    nonlinearity=nonlinearity, \n",
    "    batch_first=batch_first,\n",
    "    dropout=dropout\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape:  torch.Size([17, 29])\n",
      "hidden state shape:  torch.Size([7, 29])\n"
     ]
    }
   ],
   "source": [
    "# Unbatched input\n",
    "input = torch.randn(sequence_length, input_size)\n",
    "\n",
    "# run model\n",
    "output, hn = d_stack_rnn(input)\n",
    "\n",
    "# print output and hidden state shape\n",
    "print(\"output shape: \", output.shape)\n",
    "print(\"hidden state shape: \", hn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch first:  False\n",
      "output shape:  torch.Size([17, 32, 29])\n",
      "hidden state shape:  torch.Size([7, 32, 29])\n"
     ]
    }
   ],
   "source": [
    "# Batched input\n",
    "if batch_first:\n",
    "    input = torch.randn(batch_size, sequence_length, input_size)\n",
    "else:\n",
    "    input = torch.randn(sequence_length, batch_size, input_size)\n",
    "\n",
    "# run model\n",
    "output, hn = d_stack_rnn(input)\n",
    "\n",
    "# print output and hidden state shape\n",
    "print('Batch first: ', batch_first)\n",
    "print(\"output shape: \", output.shape)\n",
    "print(\"hidden state shape: \", hn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this Notebook we went through the main behaviors of RNN cell. We use Pytorch Built-In to guide us during the implementation. We start from a basic RNN cell highlighting the equation and tensors sizes choregraphy for aiming a prefect understanding of the recurrent process. Then we add the remaining options of RNN models, multilayer, bidirectionality, dropout. \n",
    "\n",
    "We focus on the understaning and the method to code it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
